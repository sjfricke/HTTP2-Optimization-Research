<html>

<head> 
	<title>HTTP/2 Optimization</title>
	<meta name="description" content="Research on what is the most optimal way to use HTTP2"/>
	<meta name="keywords" content="HTTP2 h2 HTTP/2 Optimization" /> 
	<link rel="stylesheet" href="css/style.css">
</head>

<body>

<div class="header center">
	<h1>HTTP/2 Optimization</h1> 
	<div> <strong> Research conducted by:</strong> Spencer Fricke, Christian Krueger, and Emmanuel Contreras Guzman at the University of Wisconsin Madison </div>
</div>
<hr>
<div class="intro">
	<h2>The Problem:</h2>
	<p>
		With the push to get people on board using HTTP/2 over HTTP/1.1 there is some uncertainty about the most <strong>optimal</strong> new way to package your website. You may be aware it was a common <strong>hack</strong> to concat all your javascript into a single file, domain shard images, and many other techniques of the same nature. We want to find what the new geneartion of <strong>optimization hacks</strong> are for HTTP/2
	</p>
</div>

<div class="section">
	<h2>The Conclusions</h2>
	<p>
		<strong> Disclaimer: </strong> There is no "standard" website! The real world involves countless of variations to websites and different client to server configuration. Theoretical values are not helpful and there is nothing consistant about internet speeds. We aimed to start with a baseline and we plan to improve this research as an ongoing project
	</p>
	<p>
		This is the tl;dr of what we found about trying to optimize your HTTP/2 site. Out reasoning for all of these are found below.
		<ul>
			<li>HTTP/2 makes it hard to have an unoptimized website!</li>
			<li>It is faster if you concatenate your files, <strong>but</strong> the performance gain serves <strong>is so small</strong> that you will gain better long-term performance caching seperate files instead</li>
			<li>List all objects (JS, CSS, Images) in a descending order of file size and not ascending</li>
			<li>Your web server (Nginx, Apache, etc) will not effect the way HTTP/2 scales</li>			
			<li>Wireless connection will not effect the way HTTP/2 scales</li>
			<li>Your server hardware will become a bottle-neck before HTTP/2 will from the overhead of too many files</li>
		</ul>
	</p>
</div>

<div class="section">
	<h2>How We Got the Results</h2>
	<p>
		We obtained all of our data in a 3 part system which can all be found on <a href="https://github.com/sjfricke/HTTP2-Optimization-Research">GitHub</a>. This was designed to allow anyone to easily in 3 steps generate their own data as we want people to help confirm our results.
		<br><br>
		<strong>Step 1 - Genearte Testing Websites</strong>
		<br>
		We have created a simple bash script that will go and generate various websites along different parameters. Since we care about the "transfering" of data using HTTP/2 only, we find it valid to fill a website with random data as the page's loading is independent of how files are sent across the network. The script is incredibly simple to use and more detail can be found in the <a href="https://github.com/sjfricke/HTTP2-Optimization-Research/tree/master/website_gen">website generator folder</a>
		<br><br>
		<strong>Step 2 - Gather HTTP/2 Request Data</strong>
		<br>
		After various methods we found that the best way to gather data is to automated the HAR file from the browsers. This desicion is made due to lack of support of headless browsers to collect the data that the network devtools offer. For Chrome we ended up using the <a href="https://developer.chrome.com/devtools/docs/debugger-protocol">Chrome Debugging Protocol</a> and the <a href="https://github.com/cyrus-and/chrome-remote-interface">NodeJS API</a> for it and ended up grabbing the <a href="https://github.com/cyrus-and/chrome-har-capturer">HAR file</a> to get the data from our request. Our <a href="https://github.com/sjfricke/HTTP2-Optimization-Research/tree/master/HAR">Headless HAR Parser</a> takes a database and the list of sites you want to run against (one is generated automatically in the website generator). Each site it grabs and gets its HAR data where it then parses it and enters all the desired data to the database. This is designed to be run as often as you want to gather all the data needed
		<br><br>
		<strong>Step 3 - Auto Generate Results and Charts from Data</strong>
		<br>
		Once you have the data to make it super easy to analyze it we created a <a href="https://github.com/sjfricke/HTTP2-Optimization-Research/tree/master/results">result generator</a> that will take data from the database and create a series of Google Charts. The scripts creates each chart as its own html page which can be used to link for reference. You can also easily take the inner data section and combine as please
	</p>
</div>

<div class="section">
	<h2>The Actual Data and Reasoning for Claims</h2>
	<span> Click on any graphs to see live version! </span><br>
	<p>
		The first task we set out to find is how HTTP/2 is effected as you increase the number of files. We created websites of 1, 2 and 4 MB where it was filled with even sized javascript files. Here is an example of what it looks like when we have it in the <strong>Same Size Structure</strong>.
		<br><div class="center"> <img src="images/File_Same_Size_Diagram.png" /> </div><br>
		After finding our results we decided to compare it to HTTP/1.1 to help us see what was really going on. Here is a graph showing how HTTP/2 scales compared to HTTP/1.1
		<br><div class="center"> <a href="../results/charts/http1_vs_http2.html" target="_blank"><img class="pageWidth" src="images/http1_vs_http2.png"/></a> </div><br>
		What is this graph shows a few very important point. First lets take a look at how much of a difference there is between where the graph is at its low point and at 5 files for both HTTP/1.1 and HTTP/2 respectifully with a 2 MB website.
		<br><div class="center"> <a href="../results/charts/http1_vs_http2.html" target="_blank"><img class="pageWidth" src="images/http1_over_50_files.png"/></a> </div><br>
		<br><div class="center"> <a href="../results/charts/http1_vs_http2.html" target="_blank"><img class="pageWidth" src="images/http2_over_50_files.png"/></a> </div><br>


	</p>
</div>

<div class="section">
	<h2>What is Next</h2>
	<p>
		aaaa
	</p>
</div>

<div class="section">
	<h2>Issues we ran into for people who want to help</h2>
	<p>
		This was a simple task that turned into a lot of wasted hours and for anyone who might care here are things to avoid for future attempts at a similar idea.
		<ul>
			<li>99% of web crawling and web-bots don't rely on the browsers implentation. To test HTTP/2 we actually need the browser to grab the files since HTTP/2 is just a spec that a browser implements. Chrome as of v57 added a headless option, but the overall realm for getting loading data is not a well developed area.</li>
			<li></li>
			<li></li>
			<li>We originally had more varation of website sizes and files, but because we made variations of 4 structures it started to take up to 2 hours just to run the Headless HAR Parser script for a single round</li>
			<li></li>
			<li></li>
		</ul>
	</p>
</div>

<body>

</html>